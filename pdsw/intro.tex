\section{Introduction}

Metadata, especially rich metadata, contains the detailed information of different entities and their relationships in HPC. These entities could be users, jobs, processes, data files, and even user-defined entities. Storing and utilizing the metadata has already provided the basic data management functionalities in most existing storage systems, including finding files, controlling file accessing, and tracing file creation and accessing time etc. We category this part of metadata as the \textit{simple metadata} since they only contain the predefined attributes about individual entity. As a contrary, \textit{rich metadata} cares more than individual predefined attributes; it may store the user-defined arbitrary attributes of entities and even their relationships. A typical example of rich metadata would be provenance (e.g. lineage). 

Provenance is well understood in the context of art or digital libraries, where it respectively refers to the documented history of an art object, or the documentation of processes in a digital object's life cycle. In the computational systems, it indicates a recording of complete history of each data piece, including the processes that generated it, the users that started the processes, and even the environment variables, parameters, and configuration files while executing. A complete provenance information supports huge amount of data management abilities. For example, the  accessing history of users reading/writing data files can help us develop a audit tool to monitor and administrate users in shared supercomputer facilities; the detailed read/write history from processes to data pieces provides a possibility to trace back the suspicious executions that generated or were based on wrong datasets; reproducibility also may be possible because we have the complete history of an execution and have a better chance to re-generate the same environment to run it again. 

With such greatness of rich metadata like provenance, current HPC platforms still lack of providing basic facility to collect, store and process the rich metadata. The challenge comes from at least three places.

\begin{itemize}

\item \textit{Storage Pressure}. Considering a leadership supercomputer, there might be millions of processes running on millions of cores accessing billions of files per second. In this case, recording the rich metadata, like detailed accessing history of each process will place a huge pressure on the storage systems. In addition, as storing rich metadata should not affect the application execution speed significantly, the resources (both network and disks bandwidth) dedicated to storing metadata are limited to use in most cases.

\item \textit{Efficient Processing}. Even we already have the perfect collected and stored rich metadata, it is still a big challenge to process them. First, as the rich metadata is large and can not be hold in one server, the distributed processing framework is necessary in most cases. Second, many use cases require complex analysis instead of simple searching or reading, so flexible processing should be provided for them.

\item \textit{Metadata diversity}. As we have described, the rich metadata could be as diverse as the usages need. They can only contain predefined attributes and relationships of entities, or be extended to any user-defined attributes and relationships. Traditionally, we used different tools (system components or users applications) to store and process part of metadata based on the specific usages. However, this introduces lots of unnecessary redundant metadata storage in different tools. For example, both the data audit tools and data verification tools need to store the file access history of users, so this metadata usually was stored twice in two applications. If we only store each metadata once in one application or component, then there will be massive cross-reference operations between them during later processing or analysis, which is inefficient.

\end{itemize}

In this paper, we proposed a idea of unifying all metadata (\textit{simple} and \textit{rich}) into a general graph-based model for HPC platform. By exploring the collection, storage, and processing of metadata, we form a practical solution named \textit{gRMM} introduced in this paper to provide such unified metadata service. 

\textit{gRMM} integrates itself closely into HPC storage systems to store rich metadata in an efficient and consistent way. In our case, the underlying storage system is Triton. The data model for storing is based on graph abstraction: all metadata for entities, attributes and relationships was abstracted as elements in a unified graph model and stored by calling the graph-based storage APIs. Moreover, by closely integrating into storage systems, \textit{gRMM} is able to trace all reads/writes on data pieces in storage system and provides detailed metadata for these data pieces automatically. Users can easily access these metadata later, combine them with information from other components to build rich metadata, and store them. 

More than this, \textit{gRMM} also contains a runtime library to help users query and process existing metadata. All these queries and processing are based on mature graph algorithms, which is expressive and efficient. In general, \textit{gRMM} aims at providing a genetic layer for easing the burden of collecting, storing and managing metadata in a modern HPC system. Working with a fault-tolerant object storage system like Triton, we could form a fully functional parallel file systems using \textit{gRMM} easily.

This paper was organized as follow: in Section II, we will first introduced the rich metadata graph model. In Section III, we introduce different metadata use cases with incremental complexity, showing how to map different use cases into proposed graph model, and solve them using facilities provided by \textit{gRMM}. In Section IV, we introduce system components and discuss the design considerations and challenges in different components. In section V, we conclude the study and list future work.