\section{Metadata Graph Prototype}
Collecting rich metadata usually requires modifications on HPC runtime as many rich metadata were generated from the runtime systems, like the jobs, processes, and read/write operations~\cite{muniswamy2006provenance,muniswamy2009layering, braun2006issues}. To help understand the attributes of a rich metadata graph in an HPC context, we exploit Darshan trace logs as a source of rich metadata in current prototyping~\cite{carns200924}.

\subsection{Mapping Strategy}
Darshan utility is a MPI library that can be linked to users' applications and generates I/O behaviors logs during executing~\cite{carns2011understanding}. Each Darshan log file represents a distinct job. The log entries of a job contain the user id who started this job, the executable file that the job was based on, the parameters of this execution, some environmental variables, and most importantly, the file access statistics of each process (ranks) inside this job (MPI program). Note that, the collected Darshan traces have been anonymized, only storing the hashed values of file names, path, user names, and job names~\cite{darshanlog2013}.

We map Darshan logs to the metadata graph defined in Section II. Basically, each unique user id indicates a User entity, each Darshan log file represents a Job, all the ranks inside a job correspond to the Processes, and both the executables and data files are abstracted as different Data Object entities. Currently, Darshan does not capture directory structure as it only stores the hashed value of file paths, so we synthetically create the simplest directory structures: data files visited by each execution are considered under the same directory, and all these directories accessed by one user are placed under one directory for each user. Based on this mapping, we were able to import a whole year' Darshan trace (\textit{2013}) on Intrepid machine into an example graph~\cite{darshanlog2013}.
%We collect basic metadata in Darshan logs as the properties too: the \textit{Type} property is set to each entity and relationship, the \textit{start\_time} and \textit{end\_time} of a job is mapped to the $start_{ts}$ and $end_{ts}$ properties of \textit{run/exe} and \textit{read/write} relationships. 

In fact, current mapping of Darshan logs emitted many common metadata due to the limitation of the data sources. For example, the logs do not have the metadata about the users; do not contain the directory structure or file permissions; and each job is based on a single executable file without configuration files and parameters. However, the generated graph still show many interesting properties of such metadata graph and offer a great potential in data management.

\subsection{Graph Size}

The first property of metadata graph is the their potential size. The graph size described here is in terms of the number of vertice, edges, and properties\footnotemark. The real storage size is based on those numbers but may vary under different data structures and storage layouts. 

\begin{table}[h]
\caption{Darshan Graph Size and Some Comparisons.}
  \label{abs}
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
Number                                                                 & Basic & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}With\\ I/O Ranks\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}With\\ Full Ranks\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}With\\ Directory\end{tabular}} \\ \hline
\begin{tabular}[c]{@{}c@{}}Vertice\end{tabular}    &      34,656 K &    41,729 K                                                                          &  147,886 K                                                                              &     147,934 K                                                                          \\ \hline
\begin{tabular}[c]{@{}c@{}}Edges\end{tabular}     &      126,488 K &  133,561 K                                                                            &     239,766 K                                                                       &    366,253 K                                                                           \\ \hline
%\begin{tabular}[c]{@{}c@{}}Properties\end{tabular} &   448,775 K    &  484,141 K                                                                              &      1,015,070 K                                                                          &           1,394,628 K                                                                    \\ \hline
%\begin{tabular}[c]{@{}c@{}}Total Size\end{tabular} &   609,918 K    & 659,431 K                                                                              &      1,402,722 K                                                                       &        1,908,815 K                                                                       \\ \hline
\hline
\hline
        & \begin{tabular}[c]{@{}c@{}}Road Graph \\ USA~\cite{demetrescu2009shortest}                                               \end{tabular} & \begin{tabular}[c]{@{}c@{}}Twitter\end{tabular} & \begin{tabular}[c]{@{}c@{}} Orkut~\cite{yang2012defining}                                              \end{tabular} & \begin{tabular}[c]{@{}c@{}}Web Page \\ Graph\end{tabular} \\ \hline
Vertice & 24 M & 645 M~\cite{twitters}                                                 & 3 M & 2.1 B~\cite{guillaume2002web}                                            \\ \hline
Edges   & 29 M & 81,364 M~\cite{twitteredge}                                                   & 117 M                                               & 15 B ~\cite{guillaume2002web}                                        \\ \hline

\end{tabular}
\end{table}

\footnotetext{K = thousand; M = million; B = billion; T = trillion}
The top half of Table~\ref{abs} shows the graph size of example metadata graph for different levels of detail. The first column considers the job as Execution entity and eliminates the all the processes (ranks) inside this job. All the I/O behaviors from different processes inside a job are considered as from the Job entity. The second column (\textit{With I/O Ranks}) records part of the ranks which have I/O operations. In many cases, this indicates the rank 0 process or the aggregators in two-phase I/O. The third column (\textit{With Full Ranks}) records all the ranks as Processes entities no matter they performed I/O or not. The last column shows the graph size with the synthetic directory structures and full ranks. This table clearly shows that increasing the level of detailed during collecting metadata will dramatically increase the graph size. So, administrators should wisely choose their metadata according to the usage. And, the user-defined relationships should be created with caution to avoid huge increasing in graph size too.

In the button half of Table~\ref{abs}, we show several typical large-scale graphs from different fields, including the social network (e.g. Twitter, Orkut), the road map graph (e.g. USA map), and the Internet web pages. By comparing with them, we can notice that, although the metadata graph is large and could easily be much larger, this kind of size has already been managed well in many existing systems.
%, and the largest and most complex graph, human brain%they are still manageable using current techniques.

\subsection{Graph Structure}
In addition to the graph size, the graph structure also matters in future storage and processing. Before discussing the graph structure, we first list a brief view of different entities of the example metadata graph in Table~\ref{smg}.

\begin{table}[h]
\caption{Statistics of Metadata Graph.}
  \label{smg}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & User  & Job  & Proc.   & Rank     & File     \\ \hline
 Num & 117      & 47,592  & 10,085,931 & 113,278,038 & 34,608,033 \\ \hline
\end{tabular}
\end{table}

This table shows different entities in metadata graph have totally different size, so in this section, we will consider them separately. Figure~\ref{doe} shows the node degree distributions of those four different entities. The $x$-axis denotes the degree, the $y$-axis shows the number of vertices which have that number of degree. Both $x$-axis and $y$-axis are `log' values. 

%The degree of a user node indicates how many jobs the user submitted in total; the degree of a job node shows how many processes it contains; the degree of each process node shows how many files are read or written by it. Data Objects both have \textit{contains/belongs} edges to other Data Objects and also have \textit{exe, read, write} edges to the Execution nodes.

   \begin{Figure}{Node degree for different entities.}[doe]
     \graphfile*[3]{exps/udegree.pdf}[User node degree]
     \graphfile*[3]{exps/jdegree.pdf}[Job node degree]\\     
     \graphfile*[3]{exps/pdegree.pdf}[Process node degree]
     \graphfile*[3]{exps/fdegree.pdf}[File node degree]
   \end{Figure}
  

%To describe the graph structure, we compared them with 
In nature, many graphs obey the \textit{skewed} power-law degree distribution~\cite{faloutsos1999power}. It means most vertices have relatively few neighbors while a few vertices have many neighbors. We can use function $\textbf{P}(d) \propto d^{-\alpha}$ to describe the probability that a vertex has degree $d$ in such graphs. Here, $\alpha$ is a positive constant that control the ``skewness'' of the degree distribution: higher $\alpha$ indicates lower density, which means vast majority of vertices are low degree. Lower $\alpha$ shows higher density and more high degree vertices. 

To compare the actual distribution with the power-law attributes, we plot three lines of the power-law distribution with different $\alpha$ values. Intuitively, we can notice that the \textit{File} nodes fit the power-law degree distribution best as Fig.~\ref{doe}(d) shows. The degree of a file node indicates the number of processes that ever read/write it. So, the distribution indicates that most files are seldom accessed, and a very small part of files are visited highly frequently. Second, as the larger $\alpha$ value ($2 \to 4$) actually fits the distribution better, which indicates the \textit{File} nodes have lower density, saying that the majority of files have low degrees. 

%This is reasonable since most jobs in HPC tend to have more processes
Other three distributions of \textit{Process}, \textit{Job} and \textit{User}, are more and more unlike the power-law distribution. For the \textit{Process} graph (Fig.~\ref{doe}(c)) , there are not enough processes with low degree. This means most processes in HPC systems tend to visit at least several files. For \textit{Job} graph (Fig.~\ref{doe}(b)), the distribution scatters randomly between the $\alpha=3$ line and $\alpha=0.5$ line. As there are only 117 users, we do not consider they fit any distribution. But from Fig~\ref{doe}(a), we still can see most users only issue several jobs, a very small number of users will issue the most jobs.

These graph structures direct the way of graph storage and processing including graph partition and storage layout. Moreover, they also lay as a basis to create synthetic graphs for evaluation test as collecting enough rich metadata in running HPC systems are still hard.

%To calculate the graph diameter and connected component, different entities and relationships are considered as the same kind of nodes and edges. For our example graph, the diameter is $x$ and the connected component number is $y$. This value can vary while we define new entity or relationship as next subsection shows.

%\subsection{More Entities or Relationships} 
%\textit{@todo: do we need this section? I guess can be eliminated?}
%The metadata graph model allows users to define their own entities and relationships, so that we are able to support arbitrary metadata management functionalities. However, intuitively, adding new entities or defining new relationships will change the graph. In this section, we take some realistic examples of user-defined entities and relationships and show how they will affect the graph size and structure.

%@todo add data

%@todo discuss the trade-off.

%\subsection{Real World Implications}
%\textit{@todo will add some comparison with some popular open graph dataset including Twitter data, Facebook data, and Yahoo dataset.}
